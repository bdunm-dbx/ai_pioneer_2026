{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd13f322-a278-464c-b6ba-6be50cd20a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This notebook configures the lab environment - NOT REQUIRED WHEN ATTENDING IN-PERSON PROCTORED TRAINING\n",
    "\n",
    "It will setup:\n",
    "- Import data into tables/volumes from the Git linked repo\n",
    "- Create a Vector Search endpoint and indices used for Knowledge Assistants/RAG\n",
    "- Set catalog/schema names and permission grants which you can adjust if configuring in your own environment\n",
    "- Creates a few generic UC (Unity Catalog) functions that you can use to experiment with\n",
    "\n",
    "The basis for this lab is a Telecom company with datasets covering operational information like billing, customers, policies, and product information as well as support related material in the form of knowledge articles and previous support phone calls + emails. There is additional supplementary marketing and branding information if you'd like to extend the lab. This corpus of information will be used to build various agentic components to demonstrate the different use cases which can be accomplished through the Databricks Data Intelligence platform.\n",
    "\n",
    "This can be run on both Classic and Serverless (env 4+) compute.\n",
    "\n",
    "---\n",
    "**For those attending the in-person training, you do not need to run this setup as the lab environment has been pre-configured**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "876fb6e9-b5b4-4196-b20d-802e137e0faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet databricks-vectorsearch\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20c571c-a8b4-4fe7-90a3-d80f1b88f2c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_catalog_and_schema(spark, catalog: str, schema: str):\n",
    "    \"\"\"Create Unity Catalog and schema if they don't exist.\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "        logger.info(f\"Ensured catalog `{catalog}` and schema `{schema}` exist.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating catalog/schema: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_csv_to_table(spark, table: str, file_path: str, catalog: str, schema: str):\n",
    "    \"\"\"Load CSV from workspace into Delta table using pandas.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading table `{table}` from {file_path}\")\n",
    "        \n",
    "        workspace_path = file_path.replace(\"file:\", \"\")\n",
    "        df = pd.read_csv(workspace_path)\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        \n",
    "        full_table = f\"{catalog}.{schema}.{table}\"\n",
    "        spark_df.write.mode(\"overwrite\").saveAsTable(full_table)\n",
    "        logger.info(f\"Created table {full_table}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV for table {table}: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_volume(spark, catalog: str, schema: str, volume: str):\n",
    "    \"\"\"Create a Unity Catalog volume if it doesn't exist.\"\"\"\n",
    "    try:\n",
    "        full_volume = f\"{catalog}.{schema}.{volume}\"\n",
    "        logger.info(f\"Creating volume {full_volume}\")\n",
    "        spark.sql(f\"CREATE VOLUME IF NOT EXISTS {full_volume}\")\n",
    "        logger.info(f\"Volume {full_volume} ensured.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating volume {full_volume}: {e}\")\n",
    "        raise\n",
    "\n",
    "def copy_folder_to_volume(workspace_folder: str, catalog: str, schema: str, volume: str, target_subfolder: str = \"\"):\n",
    "    \"\"\"Copy a workspace folder to a UC volume, preserving folder structure.\"\"\"\n",
    "    try:\n",
    "        if target_subfolder:\n",
    "            volume_path = f\"/Volumes/{catalog}/{schema}/{volume}/{target_subfolder}\"\n",
    "        else:\n",
    "            volume_path = f\"/Volumes/{catalog}/{schema}/{volume}\"\n",
    "        \n",
    "        logger.info(f\"Copying {workspace_folder} to {volume_path}\")\n",
    "        \n",
    "        # Use Python shutil to copy directory tree from workspace to volume\n",
    "        shutil.copytree(workspace_folder, volume_path, dirs_exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Successfully copied to {volume_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error copying {workspace_folder} to volume: {e}\")\n",
    "        raise\n",
    "\n",
    "def enable_cdf(spark, catalog: str, schema: str, table: str):\n",
    "    \"\"\"Enable Delta change data feed (CDF) on a table.\"\"\"\n",
    "    full_name = f\"{catalog}.{schema}.{table}\"\n",
    "    try:\n",
    "        logger.info(f\"Enabling CDF on {full_name}\")\n",
    "        spark.sql(f\"\"\"\n",
    "            ALTER TABLE {full_name}\n",
    "            SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "        \"\"\")\n",
    "        logger.info(f\"CDF enabled on {full_name}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not enable CDF on {full_name}: {e}\")\n",
    "\n",
    "def ensure_endpoint(client: VectorSearchClient, endpoint_name: str,\n",
    "                    timeout: int = 1800, poll_interval: int = 40):\n",
    "    \"\"\"Create vector search endpoint if needed and wait for it to be ready.\"\"\"\n",
    "    try:\n",
    "        eps = client.list_endpoints().get(\"endpoints\", [])\n",
    "        names = [ep[\"name\"] for ep in eps]\n",
    "        if endpoint_name not in names:\n",
    "            logger.info(f\"Creating vector search endpoint '{endpoint_name}'\")\n",
    "            client.create_endpoint(name=endpoint_name, endpoint_type=\"STANDARD\")\n",
    "        else:\n",
    "            logger.info(f\"Endpoint '{endpoint_name}' already exists\")\n",
    "\n",
    "        start = time.time()\n",
    "        while True:\n",
    "            ep_info = client.get_endpoint(endpoint_name)\n",
    "            state = ep_info.get(\"endpoint_status\", {}).get(\"state\", \"UNKNOWN\")\n",
    "            logger.info(f\"Endpoint state: {state}\")\n",
    "            if state in (\"ONLINE\", \"PROVISIONED\", \"READY\"):\n",
    "                logger.info(f\"Endpoint '{endpoint_name}' ready\")\n",
    "                break\n",
    "            if state in (\"FAILED\", \"OFFLINE\"):\n",
    "                raise RuntimeError(f\"Endpoint '{endpoint_name}' failed (state={state})\")\n",
    "            if time.time() - start > timeout:\n",
    "                raise TimeoutError(f\"Timed out waiting for endpoint '{endpoint_name}' to be ready\")\n",
    "            time.sleep(poll_interval)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error ensuring endpoint: {e}\")\n",
    "        raise\n",
    "\n",
    "def index_exists(client: VectorSearchClient, endpoint_name: str, index_name: str) -> bool:\n",
    "    \"\"\"Check if a vector search index already exists on the endpoint.\"\"\"\n",
    "    try:\n",
    "        resp = client.list_indexes(name=endpoint_name)\n",
    "        for idx in resp.get(\"indexes\", []):\n",
    "            if idx.get(\"name\") == index_name:\n",
    "                return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not list indexes (assuming none exist): {e}\")\n",
    "        return False\n",
    "\n",
    "def create_delta_index(client: VectorSearchClient, endpoint_name: str,\n",
    "                       source_table: str, index_name: str,\n",
    "                       pipeline_type: str, primary_key: str, embedding_column: str):\n",
    "    \"\"\"Create a delta sync vector search index with automatic CDF retry if needed.\"\"\"\n",
    "    if index_exists(client, endpoint_name, index_name):\n",
    "        logger.info(f\"Index '{index_name}' already exists, skipping\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Creating index '{index_name}' on {source_table}\")\n",
    "        client.create_delta_sync_index(\n",
    "            endpoint_name=endpoint_name,\n",
    "            index_name=index_name,\n",
    "            source_table_name=source_table,\n",
    "            pipeline_type=pipeline_type,\n",
    "            primary_key=primary_key,\n",
    "            embedding_source_column=embedding_column,\n",
    "            embedding_model_endpoint_name=\"databricks-gte-large-en\"\n",
    "        )\n",
    "        logger.info(f\"Index '{index_name}' created successfully\")\n",
    "    except Exception as ex:\n",
    "        msg = str(ex)\n",
    "        if \"does not have change data feed enabled\" in msg:\n",
    "            logger.info(f\"CDF not enabled on {source_table}, attempting to enable and retry\")\n",
    "            parts = source_table.split(\".\")\n",
    "            if len(parts) == 3:\n",
    "                cat, sch, tbl = parts\n",
    "                try:\n",
    "                    enable_cdf(spark, cat, sch, tbl)\n",
    "                    logger.info(f\"Retrying index creation for '{index_name}'\")\n",
    "                    client.create_delta_sync_index(\n",
    "                        endpoint_name=endpoint_name,\n",
    "                        index_name=index_name,\n",
    "                        source_table_name=source_table,\n",
    "                        pipeline_type=pipeline_type,\n",
    "                        primary_key=primary_key,\n",
    "                        embedding_source_column=embedding_column,\n",
    "                        embedding_model_endpoint_name=\"databricks-gte-large-en\"\n",
    "                    )\n",
    "                    logger.info(f\"Index '{index_name}' created after retry\")\n",
    "                    return\n",
    "                except Exception as e2:\n",
    "                    logger.error(f\"Retry failed for index '{index_name}': {e2}\")\n",
    "            else:\n",
    "                logger.error(f\"Could not parse table name '{source_table}' for CDF retry\")\n",
    "        logger.error(f\"Failed to create index '{index_name}': {msg}\")\n",
    "        raise\n",
    "\n",
    "def main(spark):\n",
    "    \"\"\"Main setup function for AI Pioneer lab.\"\"\"\n",
    "    catalog = \"ai_pioneer\"\n",
    "    schema = \"lab_data\"\n",
    "    attendee_schema = \"attendee_catalog\"\n",
    "    endpoint_name = \"ai_pioneer_vs_endpoint\"\n",
    "    \n",
    "    # Determine data folder path from notebook location\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    logger.info(f\"Notebook path: {notebook_path}\")\n",
    "    \n",
    "    notebook_dir = os.path.dirname(notebook_path)\n",
    "    data_folder = f\"file:/Workspace{notebook_dir}/Data\"\n",
    "    logger.info(f\"Using data folder: {data_folder}\")\n",
    "    \n",
    "    csv_files = {\n",
    "        \"billing\": f\"{data_folder}/billing.csv\",\n",
    "        \"customers\": f\"{data_folder}/customers.csv\",\n",
    "        \"knowledge_base\": f\"{data_folder}/knowledge_base.csv\",\n",
    "        \"support_tickets\": f\"{data_folder}/support_tickets.csv\",\n",
    "        \"cust_service_data\": f\"{data_folder}/cust_service_data.csv\",\n",
    "        \"policies\": f\"{data_folder}/policies.csv\",\n",
    "        \"product_docs\": f\"{data_folder}/product_docs.csv\"\n",
    "    }\n",
    "\n",
    "    # Step 1: Create catalog and schema\n",
    "    create_catalog_and_schema(spark, catalog, schema)\n",
    "    create_catalog_and_schema(spark, catalog, attendee_schema)\n",
    "\n",
    "    # Step 2: Load CSV files into Delta tables\n",
    "    for tbl, file_path in csv_files.items():\n",
    "        load_csv_to_table(spark, tbl, file_path, catalog, schema)\n",
    "\n",
    "    # Step 3: Create UC volumes and copy unstructured data folders\n",
    "    workspace_data_folder = data_folder.replace(\"file:\", \"\")\n",
    "    \n",
    "    # Create volume for call transcripts\n",
    "    volume_calls = \"customer_call_transcripts\"\n",
    "    create_volume(spark, catalog, schema, volume_calls)\n",
    "    logger.info(\"Copying call transcripts to volume\")\n",
    "    copy_folder_to_volume(\n",
    "        workspace_folder=f\"{workspace_data_folder}/call_transcripts\",\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        volume=volume_calls,\n",
    "        target_subfolder=\"\"\n",
    "    )\n",
    "    \n",
    "    # Create volume for customer emails\n",
    "    volume_emails = \"customer_emails\"\n",
    "    create_volume(spark, catalog, schema, volume_emails)\n",
    "    logger.info(\"Copying email correspondence to volume\")\n",
    "    copy_folder_to_volume(\n",
    "        workspace_folder=f\"{workspace_data_folder}/email_correspondence\",\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        volume=volume_emails,\n",
    "        target_subfolder=\"\"\n",
    "    )\n",
    "    \n",
    "    # Create volume for marketing materials\n",
    "    volume_marketing = \"marketing\"\n",
    "    create_volume(spark, catalog, schema, volume_marketing)\n",
    "    logger.info(\"Copying marketing materials to volume\")\n",
    "    copy_folder_to_volume(\n",
    "        workspace_folder=f\"{workspace_data_folder}/marketing_materials\",\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        volume=volume_marketing,\n",
    "        target_subfolder=\"\"\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Unstructured data copied to volumes: {volume_calls}, {volume_emails}, {volume_marketing}\")\n",
    "\n",
    "    # Step 4: Create vector search endpoint\n",
    "    vs_client = VectorSearchClient()\n",
    "    ensure_endpoint(vs_client, endpoint_name)\n",
    "\n",
    "    # Step 5: Enable CDF on source tables\n",
    "    try:\n",
    "        enable_cdf(spark, catalog, schema, \"knowledge_base\")\n",
    "        enable_cdf(spark, catalog, schema, \"support_tickets\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed enabling CDF for one or more tables: {e}\")\n",
    "\n",
    "    # Step 6: Create vector search indexes\n",
    "    kb_table = f\"{catalog}.{schema}.knowledge_base\"\n",
    "    st_table = f\"{catalog}.{schema}.support_tickets\"\n",
    "    kb_index = f\"{catalog}.{schema}.knowledge_base_index\"\n",
    "    st_index = f\"{catalog}.{schema}.support_tickets_index\"\n",
    "    \n",
    "    kb_pk = \"kb_id\"\n",
    "    kb_text = \"formatted_content\"\n",
    "    st_pk = \"ticket_id\"\n",
    "    st_text = \"formatted_content\"\n",
    "\n",
    "    create_delta_index(vs_client, endpoint_name, kb_table, kb_index, \"TRIGGERED\", kb_pk, kb_text)\n",
    "    create_delta_index(vs_client, endpoint_name, st_table, st_index, \"TRIGGERED\", st_pk, st_text)\n",
    "\n",
    "    logger.info(\"Setup complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main(spark)\n",
    "    except NameError:\n",
    "        logger.error(\"This script must run in Databricks environment where `spark` is defined.\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Setup script failed: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c48d69-14b1-4c3e-b21d-daa6164821a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grants on catalog/schema"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Admin permissions\n",
    "GRANT ALL PRIVILEGES ON CATALOG ai_pioneer TO `workshop_admins`;\n",
    "GRANT MANAGE ON CATALOG ai_pioneer TO `workshop_admins`;\n",
    "\n",
    "-- Attendee permissions\n",
    "GRANT USE CATALOG ON CATALOG ai_pioneer TO `workshop_users`;\n",
    "GRANT CREATE SCHEMA ON CATALOG ai_pioneer TO `workshop_users`;\n",
    "\n",
    "-- Lab data schema\n",
    "GRANT USE SCHEMA ON SCHEMA ai_pioneer.lab_data TO `workshop_users`;\n",
    "GRANT EXECUTE ON SCHEMA ai_pioneer.lab_data TO `workshop_users`;\n",
    "GRANT READ VOLUME ON SCHEMA ai_pioneer.lab_data TO `workshop_users`; \n",
    "GRANT SELECT ON SCHEMA ai_pioneer.lab_data TO `workshop_users`;\n",
    "\n",
    "-- Experimentation schema for attendees where they have full rights\n",
    "GRANT USE SCHEMA ON SCHEMA ai_pioneer.attendee_catalog TO `workshop_users`;\n",
    "GRANT APPLY TAG ON SCHEMA ai_pioneer.attendee_catalog TO `workshop_users`;\n",
    "GRANT EXECUTE ON SCHEMA ai_pioneer.attendee_catalog TO `workshop_users`;\n",
    "GRANT READ VOLUME ON SCHEMA ai_pioneer.attendee_catalog TO `workshop_users`;\n",
    "GRANT SELECT ON SCHEMA ai_pioneer.attendee_catalog TO `workshop_users`;\n",
    "GRANT MODIFY ON SCHEMA ai_pioneer.attendee_catalog TO `workshop_users`;\n",
    "GRANT REFRESH ON SCHEMA ai_pioneer.attendee_catalog TO `workshop_users`;\n",
    "GRANT CREATE FUNCTION ON SCHEMA ai_pioneer.attendee_catalog TO `workshop_users`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88cc0b47-92e4-41c9-a82f-15db2a6371f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Permissions on VS endpoint"
    }
   },
   "outputs": [],
   "source": [
    "# Set manually using UI access lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ceeddb-1c7f-40d6-868c-31efa9061998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Create generic UC functions to demonstrate capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074557ef-c514-4f3b-916f-31c93ccdb3a1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrieve current weather"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ai_pioneer.lab_data.get_current_weather(\n",
    "  latitude FLOAT,\n",
    "  longitude FLOAT\n",
    ")\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "COMMENT 'Fetches the current weather for a specific lat/long coordinate using the Open-Meteo free API.'\n",
    "AS $$\n",
    "  import requests\n",
    "  import json\n",
    "  \n",
    "  # Free public API (no key required)\n",
    "  url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "  \n",
    "  try:\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    current = data.get(\"current_weather\", {})\n",
    "    temp = current.get(\"temperature\")\n",
    "    speed = current.get(\"windspeed\")\n",
    "    \n",
    "    return json.dumps({\n",
    "      \"temperature_celsius\": temp,\n",
    "      \"wind_speed_kmh\": speed,\n",
    "      \"location\": f\"{latitude}, {longitude}\"\n",
    "    })\n",
    "  except Exception as e:\n",
    "    return f\"Error fetching weather: {str(e)}\"\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f4a766-8098-4fd8-b098-bf0800fb128b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Determine number of business days vs weekend days"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ai_pioneer.lab_data.get_business_day_offset(\n",
    "  start_date DATE, \n",
    "  days_to_add INT\n",
    ")\n",
    "RETURNS DATE\n",
    "LANGUAGE PYTHON\n",
    "COMMENT 'Adds a number of business days (skipping weekends) to a start date. Useful for calculating SLA deadlines.'\n",
    "AS $$\n",
    "  from datetime import timedelta\n",
    "  \n",
    "  current_date = start_date\n",
    "  added_days = 0\n",
    "  \n",
    "  while added_days < days_to_add:\n",
    "    current_date += timedelta(days=1)\n",
    "    # 5 = Saturday, 6 = Sunday\n",
    "    if current_date.weekday() < 5:\n",
    "      added_days += 1\n",
    "      \n",
    "  return current_date\n",
    "$$;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7510536352933568,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "00_ConfigLab",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
