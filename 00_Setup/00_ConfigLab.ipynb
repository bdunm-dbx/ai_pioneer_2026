{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd13f322-a278-464c-b6ba-6be50cd20a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add some notes here -- also, this probably isn't surfaced to users and just people setting up.\n",
    "\n",
    "Runs on Serverless Env 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "876fb6e9-b5b4-4196-b20d-802e137e0faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet databricks-vectorsearch\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20c571c-a8b4-4fe7-90a3-d80f1b88f2c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_catalog_and_schema(spark, catalog: str, schema: str):\n",
    "    \"\"\"Create Unity Catalog and schema if they don't exist.\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "        logger.info(f\"Ensured catalog `{catalog}` and schema `{schema}` exist.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating catalog/schema: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_csv_to_table(spark, table: str, file_path: str, catalog: str, schema: str):\n",
    "    \"\"\"Load CSV from workspace into Delta table using pandas.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading table `{table}` from {file_path}\")\n",
    "        \n",
    "        workspace_path = file_path.replace(\"file:\", \"\")\n",
    "        df = pd.read_csv(workspace_path)\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        \n",
    "        full_table = f\"{catalog}.{schema}.{table}\"\n",
    "        spark_df.write.mode(\"overwrite\").saveAsTable(full_table)\n",
    "        logger.info(f\"Created table {full_table}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV for table {table}: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_volume(spark, catalog: str, schema: str, volume: str):\n",
    "    \"\"\"Create a Unity Catalog volume if it doesn't exist.\"\"\"\n",
    "    try:\n",
    "        full_volume = f\"{catalog}.{schema}.{volume}\"\n",
    "        logger.info(f\"Creating volume {full_volume}\")\n",
    "        spark.sql(f\"CREATE VOLUME IF NOT EXISTS {full_volume}\")\n",
    "        logger.info(f\"Volume {full_volume} ensured.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating volume {full_volume}: {e}\")\n",
    "        raise\n",
    "\n",
    "def copy_folder_to_volume(workspace_folder: str, catalog: str, schema: str, volume: str, target_subfolder: str = \"\"):\n",
    "    \"\"\"Copy a workspace folder to a UC volume, preserving folder structure.\"\"\"\n",
    "    try:\n",
    "        if target_subfolder:\n",
    "            volume_path = f\"/Volumes/{catalog}/{schema}/{volume}/{target_subfolder}\"\n",
    "        else:\n",
    "            volume_path = f\"/Volumes/{catalog}/{schema}/{volume}\"\n",
    "        \n",
    "        logger.info(f\"Copying {workspace_folder} to {volume_path}\")\n",
    "        \n",
    "        # Use Python shutil to copy directory tree from workspace to volume\n",
    "        shutil.copytree(workspace_folder, volume_path, dirs_exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Successfully copied to {volume_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error copying {workspace_folder} to volume: {e}\")\n",
    "        raise\n",
    "\n",
    "def enable_cdf(spark, catalog: str, schema: str, table: str):\n",
    "    \"\"\"Enable Delta change data feed (CDF) on a table.\"\"\"\n",
    "    full_name = f\"{catalog}.{schema}.{table}\"\n",
    "    try:\n",
    "        logger.info(f\"Enabling CDF on {full_name}\")\n",
    "        spark.sql(f\"\"\"\n",
    "            ALTER TABLE {full_name}\n",
    "            SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "        \"\"\")\n",
    "        logger.info(f\"CDF enabled on {full_name}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not enable CDF on {full_name}: {e}\")\n",
    "\n",
    "def ensure_endpoint(client: VectorSearchClient, endpoint_name: str,\n",
    "                    timeout: int = 1200, poll_interval: int = 20):\n",
    "    \"\"\"Create vector search endpoint if needed and wait for it to be ready.\"\"\"\n",
    "    try:\n",
    "        eps = client.list_endpoints().get(\"endpoints\", [])\n",
    "        names = [ep[\"name\"] for ep in eps]\n",
    "        if endpoint_name not in names:\n",
    "            logger.info(f\"Creating vector search endpoint '{endpoint_name}'\")\n",
    "            client.create_endpoint(name=endpoint_name, endpoint_type=\"STANDARD\")\n",
    "        else:\n",
    "            logger.info(f\"Endpoint '{endpoint_name}' already exists\")\n",
    "\n",
    "        start = time.time()\n",
    "        while True:\n",
    "            ep_info = client.get_endpoint(endpoint_name)\n",
    "            state = ep_info.get(\"endpoint_status\", {}).get(\"state\", \"UNKNOWN\")\n",
    "            logger.info(f\"Endpoint state: {state}\")\n",
    "            if state in (\"ONLINE\", \"PROVISIONED\", \"READY\"):\n",
    "                logger.info(f\"Endpoint '{endpoint_name}' ready\")\n",
    "                break\n",
    "            if state in (\"FAILED\", \"OFFLINE\"):\n",
    "                raise RuntimeError(f\"Endpoint '{endpoint_name}' failed (state={state})\")\n",
    "            if time.time() - start > timeout:\n",
    "                raise TimeoutError(f\"Timed out waiting for endpoint '{endpoint_name}' to be ready\")\n",
    "            time.sleep(poll_interval)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error ensuring endpoint: {e}\")\n",
    "        raise\n",
    "\n",
    "def index_exists(client: VectorSearchClient, endpoint_name: str, index_name: str) -> bool:\n",
    "    \"\"\"Check if a vector search index already exists on the endpoint.\"\"\"\n",
    "    try:\n",
    "        resp = client.list_indexes(name=endpoint_name)\n",
    "        for idx in resp.get(\"indexes\", []):\n",
    "            if idx.get(\"name\") == index_name:\n",
    "                return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not list indexes (assuming none exist): {e}\")\n",
    "        return False\n",
    "\n",
    "def create_delta_index(client: VectorSearchClient, endpoint_name: str,\n",
    "                       source_table: str, index_name: str,\n",
    "                       pipeline_type: str, primary_key: str, embedding_column: str):\n",
    "    \"\"\"Create a delta sync vector search index with automatic CDF retry if needed.\"\"\"\n",
    "    if index_exists(client, endpoint_name, index_name):\n",
    "        logger.info(f\"Index '{index_name}' already exists, skipping\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Creating index '{index_name}' on {source_table}\")\n",
    "        client.create_delta_sync_index(\n",
    "            endpoint_name=endpoint_name,\n",
    "            index_name=index_name,\n",
    "            source_table_name=source_table,\n",
    "            pipeline_type=pipeline_type,\n",
    "            primary_key=primary_key,\n",
    "            embedding_source_column=embedding_column,\n",
    "            embedding_model_endpoint_name=\"databricks-gte-large-en\"\n",
    "        )\n",
    "        logger.info(f\"Index '{index_name}' created successfully\")\n",
    "    except Exception as ex:\n",
    "        msg = str(ex)\n",
    "        if \"does not have change data feed enabled\" in msg:\n",
    "            logger.info(f\"CDF not enabled on {source_table}, attempting to enable and retry\")\n",
    "            parts = source_table.split(\".\")\n",
    "            if len(parts) == 3:\n",
    "                cat, sch, tbl = parts\n",
    "                try:\n",
    "                    enable_cdf(spark, cat, sch, tbl)\n",
    "                    logger.info(f\"Retrying index creation for '{index_name}'\")\n",
    "                    client.create_delta_sync_index(\n",
    "                        endpoint_name=endpoint_name,\n",
    "                        index_name=index_name,\n",
    "                        source_table_name=source_table,\n",
    "                        pipeline_type=pipeline_type,\n",
    "                        primary_key=primary_key,\n",
    "                        embedding_source_column=embedding_column,\n",
    "                        embedding_model_endpoint_name=\"databricks-gte-large-en\"\n",
    "                    )\n",
    "                    logger.info(f\"Index '{index_name}' created after retry\")\n",
    "                    return\n",
    "                except Exception as e2:\n",
    "                    logger.error(f\"Retry failed for index '{index_name}': {e2}\")\n",
    "            else:\n",
    "                logger.error(f\"Could not parse table name '{source_table}' for CDF retry\")\n",
    "        logger.error(f\"Failed to create index '{index_name}': {msg}\")\n",
    "        raise\n",
    "\n",
    "def main(spark):\n",
    "    \"\"\"Main setup function for AI Pioneer lab.\"\"\"\n",
    "    catalog = \"ai_pioneer\"\n",
    "    schema = \"lab_data\"\n",
    "    attendee_schema = \"attendee_catalog\"\n",
    "    endpoint_name = \"ai_pioneer_vs_endpoint\"\n",
    "    \n",
    "    # Determine data folder path from notebook location\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    logger.info(f\"Notebook path: {notebook_path}\")\n",
    "    \n",
    "    notebook_dir = os.path.dirname(notebook_path)\n",
    "    data_folder = f\"file:/Workspace{notebook_dir}/Data\"\n",
    "    logger.info(f\"Using data folder: {data_folder}\")\n",
    "    \n",
    "    csv_files = {\n",
    "        \"billing\": f\"{data_folder}/billing.csv\",\n",
    "        \"customers\": f\"{data_folder}/customers.csv\",\n",
    "        \"knowledge_base\": f\"{data_folder}/knowledge_base.csv\",\n",
    "        \"support_tickets\": f\"{data_folder}/support_tickets.csv\",\n",
    "        \"cust_service_data\": f\"{data_folder}/cust_service_data.csv\",\n",
    "        \"policies\": f\"{data_folder}/policies.csv\",\n",
    "        \"product_docs\": f\"{data_folder}/product_docs.csv\"\n",
    "    }\n",
    "\n",
    "    # Step 1: Create catalog and schema\n",
    "    create_catalog_and_schema(spark, catalog, schema)\n",
    "    create_catalog_and_schema(spark, catalog, attendee_schema)\n",
    "\n",
    "    # Step 2: Load CSV files into Delta tables\n",
    "    for tbl, file_path in csv_files.items():\n",
    "        load_csv_to_table(spark, tbl, file_path, catalog, schema)\n",
    "\n",
    "    # Step 3: Create UC volumes and copy unstructured data folders\n",
    "    workspace_data_folder = data_folder.replace(\"file:\", \"\")\n",
    "    \n",
    "    # Create volume for call transcripts\n",
    "    volume_calls = \"customer_call_transcripts\"\n",
    "    create_volume(spark, catalog, schema, volume_calls)\n",
    "    logger.info(\"Copying call transcripts to volume\")\n",
    "    copy_folder_to_volume(\n",
    "        workspace_folder=f\"{workspace_data_folder}/call_transcripts\",\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        volume=volume_calls,\n",
    "        target_subfolder=\"\"\n",
    "    )\n",
    "    \n",
    "    # Create volume for customer emails\n",
    "    volume_emails = \"customer_emails\"\n",
    "    create_volume(spark, catalog, schema, volume_emails)\n",
    "    logger.info(\"Copying email correspondence to volume\")\n",
    "    copy_folder_to_volume(\n",
    "        workspace_folder=f\"{workspace_data_folder}/email_correspondence\",\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        volume=volume_emails,\n",
    "        target_subfolder=\"\"\n",
    "    )\n",
    "    \n",
    "    # Create volume for marketing materials\n",
    "    volume_marketing = \"marketing\"\n",
    "    create_volume(spark, catalog, schema, volume_marketing)\n",
    "    logger.info(\"Copying marketing materials to volume\")\n",
    "    copy_folder_to_volume(\n",
    "        workspace_folder=f\"{workspace_data_folder}/marketing_materials\",\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        volume=volume_marketing,\n",
    "        target_subfolder=\"\"\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Unstructured data copied to volumes: {volume_calls}, {volume_emails}, {volume_marketing}\")\n",
    "\n",
    "    # Step 4: Create vector search endpoint\n",
    "    vs_client = VectorSearchClient()\n",
    "    ensure_endpoint(vs_client, endpoint_name)\n",
    "\n",
    "    # Step 5: Enable CDF on source tables\n",
    "    try:\n",
    "        enable_cdf(spark, catalog, schema, \"knowledge_base\")\n",
    "        enable_cdf(spark, catalog, schema, \"support_tickets\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed enabling CDF for one or more tables: {e}\")\n",
    "\n",
    "    # Step 6: Create vector search indexes\n",
    "    kb_table = f\"{catalog}.{schema}.knowledge_base\"\n",
    "    st_table = f\"{catalog}.{schema}.support_tickets\"\n",
    "    kb_index = f\"{catalog}.{schema}.knowledge_base_index\"\n",
    "    st_index = f\"{catalog}.{schema}.support_tickets_index\"\n",
    "    \n",
    "    kb_pk = \"kb_id\"\n",
    "    kb_text = \"formatted_content\"\n",
    "    st_pk = \"ticket_id\"\n",
    "    st_text = \"formatted_content\"\n",
    "\n",
    "    create_delta_index(vs_client, endpoint_name, kb_table, kb_index, \"TRIGGERED\", kb_pk, kb_text)\n",
    "    create_delta_index(vs_client, endpoint_name, st_table, st_index, \"TRIGGERED\", st_pk, st_text)\n",
    "\n",
    "    logger.info(\"Setup complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main(spark)\n",
    "    except NameError:\n",
    "        logger.error(\"This script must run in Databricks environment where `spark` is defined.\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Setup script failed: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c48d69-14b1-4c3e-b21d-daa6164821a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grants on catalog/schema"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Admin permissions\n",
    "GRANT ALL PRIVILEGES ON CATALOG ai_pioneer TO `workshop_admins`;\n",
    "GRANT MANAGE ON CATALOG ai_pioneer TO `workshop_admins`;\n",
    "\n",
    "-- Attendee permissions\n",
    "GRANT USE CATALOG ON CATALOG ai_pioneer TO `workshop_users`;\n",
    "\n",
    "-- Lab data schema\n",
    "GRANT USE SCHEMA ON SCHEMA ai_pioneer.lab_data TO `workshop_users`;\n",
    "GRANT READ VOLUME ON SCHEMA ai_pioneer.lab_data TO `workshop_users`; \n",
    "GRANT SELECT ON SCHEMA ai_pioneer.lab_data TO `workshop_users`;\n",
    "GRANT EXECUTE ON SCHEMA ai_pioneer.lab_data TO `workshop_users`;\n",
    "\n",
    "-- Experimentation schema for attendees where they have full rights\n",
    "GRANT ALL PRIVILEGES ON SCHEMA ai_pioneer.attendee_catalog TO `workshop_users`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88cc0b47-92e4-41c9-a82f-15db2a6371f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Permissions on VS endpoint"
    }
   },
   "outputs": [],
   "source": [
    "# Set manually using UI access lists"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6455014078984376,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_ConfigLab",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
