{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f007469-625e-4dd2-a3ea-e69eb717e48b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hands-On Lab: Building Agent Systems with Databricks\n",
    "\n",
    "## MLflow Evaluations for GenAI Agents on Databricks\n",
    ">\n",
    "> **Goal**: Learn how to evaluate ML and GenAI models using **MLflow Evaluations** in Databricks, understand where results live in the UI, and how to operationalize evaluation workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0811ca85-5525-4451-9e7b-4e2ba83395df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Evaluations Overview\n",
    ">\n",
    "> **Goal**: Learn how to evaluate **GenAI / LLM applications** using **MLflow Evaluations** in Databricks, understand how these evaluations differ from classic ML, and where to inspect results in the Databricks UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "355d4218-8123-4fac-82e5-f8a07ba95329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. What Are MLflow Evaluations?\n",
    "\n",
    "MLflow Evaluations provide a **standardized framework** for evaluating:\n",
    "\n",
    "- **GenAI / LLM applications** (text quality, relevance, safety)\n",
    "- **Classic ML models** (classification, regression)\n",
    "\n",
    "For agentic workflows, evaluations focus less on \"accuracy\" and more on **judgment-based metrics** that assess output quality.\n",
    "\n",
    "Key MLflow Agent Evaluation benefits:\n",
    "\n",
    "- Native support for **LLM-as-a-judge** evaluators\n",
    "- Row-level qualitative and quantitative results\n",
    "- Reproducible evaluation artifacts tied to prompts, models, and data\n",
    "\n",
    "In agent evaluations, the nature of evaluations shift from *“Is this response good?”* rather than *“Is this prediction correct?”*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e45dc4f1-1a0a-45c7-9379-ecb5c8114ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Where MLflow Evaluations Fit in a GenAI Workflow\n",
    "\n",
    "In GenAI applications, MLflow Evaluations typically happen:\n",
    "\n",
    "1. After prompt, chain, or agent changes\n",
    "2. Before deploying or updating a serving endpoint\n",
    "3. Continuously after deployment for quality regression detection\n",
    "\n",
    "They integrate with:\n",
    "\n",
    "- MLflow Experiments (prompt + model iteration)\n",
    "- Unity Catalog Model Registry (governed promotion)\n",
    "- Databricks Model Serving (endpoint-based evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32ddf6e4-c9a1-470e-bd2f-5b72734912f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](./_images/mlflow3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "816a98b6-ad3a-486b-94d7-8a49aa3c47ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Navigating through the user interface\n",
    "\n",
    "1. MLflow is available on Databricks out of the box\n",
    "2. Key Components of the MLflow interface consists of:\n",
    "-   Experiments\n",
    "-   Models\n",
    "-   Serving\n",
    "\n",
    "Let's first navigate to **Experiments** in the left-hand navigation\n",
    "\n",
    "![](./_images/experiments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d7c84e-61d6-4bb5-86d6-9672f68a2505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Running an Evaluation (High-Level Flow)\n",
    "\n",
    "At a high level, running an evaluation involves:\n",
    "\n",
    "- A trained model (or endpoint)\n",
    "- An evaluation dataset\n",
    "- An evaluation configuration (metrics, evaluators)\n",
    "\n",
    "MLflow handles metric computation and logging automatically. In Databricks, Tracing can be turned on with a single command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a65094f2-a02c-470b-bf75-6336d4d1207a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](./_images/tracing_log.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48080cf2-cd9b-4df3-82f7-b15de1b4b140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Evaluating a GenAI / LLM Application\n",
    "\n",
    "### What Is Being Evaluated?\n",
    "\n",
    "In GenAI, you are typically evaluating:\n",
    "\n",
    "- Prompt templates\n",
    "- Foundation models or fine-tuned LLMs\n",
    "- Chains, agents, or tools\n",
    "- End-to-end application outputs\n",
    "\n",
    "### Inputs to a GenAI Evaluation\n",
    "\n",
    "- Input prompts or questions\n",
    "- Optional reference answers\n",
    "- Model or endpoint outputs\n",
    "- One or more evaluators (LLM, rule-based, or human)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92523e70-8b3c-4250-bd45-a1fe3237c058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](./_images/tracing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af4a1218-15a4-447b-acc1-fcb223cc71b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Let's put this to the test and try evaluating our multi-agent supervisor and knowledge assistant more. Below is a list of sample questions for you to ask!\n",
    "\n",
    "- Why am I experiencing no service or intermittent connectivity on my device?\n",
    "- Why are my data speeds slow or my calls dropping, and what can we check to fix this?\n",
    "- Is there an outage or known network issue in my area affecting my service?\n",
    "- Could this issue be caused by my device rather than the network, and how can we tell?\n",
    "- Is there a problem with my activation, SIM/eSIM, or number porting?\n",
    "- Can you explain these charges on my bill or whether my plan is set up correctly?\n",
    "- Why does this issue keep happening even after I’ve contacted support before?\n",
    "- Are there any troubleshooting steps I may have missed that could fix this quickly?\n",
    "- Does this issue need to be escalated, and what will that process look like?\n",
    "- Is there a way I could resolve this issue myself in the future without calling support?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1778683b-0f92-4902-9808-9cbe69d541b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Exploring Evaluation Results in the UI\n",
    "\n",
    "After the evaluation completes:\n",
    "\n",
    "1. Navigate to the **Experiment Runs** page\n",
    "2. Select the run associated with the Knowledge Assistant evaluation\n",
    "3. Review:\n",
    "   - Logs/Traces\n",
    "   - Scorers\n",
    "   - Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ddffab-ec2b-4312-b2cd-4d9ae84963be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. GenAI Evaluators Explained\n",
    "\n",
    "MLflow supports multiple evaluator types for GenAI:\n",
    "\n",
    "### Automated LLM-as-a-Judge Evaluators\n",
    "\n",
    "- Answer relevance\n",
    "- Correctness (vs reference)\n",
    "- Faithfulness / groundedness\n",
    "\n",
    "### Safety & Policy Evaluators\n",
    "\n",
    "- Toxicity\n",
    "- Bias\n",
    "- Harmful content\n",
    "\n",
    "### Custom & Rule-Based Evaluators\n",
    "\n",
    "- Regex or keyword checks\n",
    "- Business logic validations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f29b9b3-46dd-48be-94dc-f527044d4984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Using GenAI Evaluations for Governance & Promotion\n",
    "\n",
    "GenAI evaluations are critical for:\n",
    "\n",
    "- Justifying prompt or model changes\n",
    "- Preventing silent quality regressions\n",
    "- Enabling governed promotion to Production\n",
    "\n",
    "UI flow:\n",
    "\n",
    "1. Register the model or GenAI pipeline from the run\n",
    "2. View evaluation artifacts directly on the model version\n",
    "3. Use metrics and qualitative examples during stage reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f1af61-8d5e-4f34-bd7a-d521d1fa4ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Best Practices Summary\n",
    "\n",
    "**Standardize Evaluations**\n",
    "- Use consistent datasets and metrics\n",
    "\n",
    "**Log Early and Often**\n",
    "- Treat evaluations as first-class artifacts\n",
    "\n",
    "**Automate**\n",
    "- Include evaluations in CI/CD or training pipelines\n",
    "\n",
    "**Use Human Review Strategically**\n",
    "- Especially for GenAI edge cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f243417-023a-40ed-b7fb-f03df0bee0d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Common GenAI Evaluation Pitfalls\n",
    "\n",
    "- Relying on a single LLM judge\n",
    "- Not versioning prompts or evaluation data\n",
    "- Evaluating only averages instead of per-row failures\n",
    "- Ignoring qualitative review for edge cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e1eeb50-60e0-48dc-9719-5a108243134c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 12. Wrap-Up & Next Steps\n",
    "\n",
    "In this workshop, we covered:\n",
    "\n",
    "- How MLflow Evaluations support GenAI workflows\n",
    "- How GenAI evaluation differs from classic ML\n",
    "- Where to inspect qualitative and quantitative results in the UI\n",
    "- How to use evaluations for governance and deployment decisions\n",
    "\n",
    "**Next steps**:\n",
    "\n",
    "- Add GenAI evaluations to prompt iteration loops\n",
    "- Combine automated and human review\n",
    "- Use evaluation artifacts to gate production changes"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Agent_Eval",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
