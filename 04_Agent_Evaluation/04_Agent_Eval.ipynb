{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f007469-625e-4dd2-a3ea-e69eb717e48b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hands-On Lab: Building Agent Systems with Databricks\n",
    "\n",
    "## MLflow Evaluations for GenAI Agents on Databricks\n",
    "\n",
    "> **Audience**: Data Scientists, ML Engineers, and Platform Engineers\n",
    ">\n",
    "> **Goal**: Learn how to evaluate ML and GenAI models using **MLflow Evaluations** in Databricks, understand where results live in the UI, and how to operationalize evaluation workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0811ca85-5525-4451-9e7b-4e2ba83395df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Workshop Overview\n",
    "\n",
    "> **Audience**: GenAI Engineers, Applied AI Scientists, ML Engineers\n",
    ">\n",
    "> **Goal**: Learn how to evaluate **GenAI / LLM applications** using **MLflow Evaluations** in Databricks, understand how these evaluations differ from classic ML, and where to inspect results in the Databricks UI.\n",
    "\n",
    "This workshop is **GenAI-first**. Classic ML evaluations are covered briefly for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "355d4218-8123-4fac-82e5-f8a07ba95329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. What Are MLflow Evaluations?\n",
    "\n",
    "MLflow Evaluations provide a **standardized framework** for evaluating:\n",
    "\n",
    "- **GenAI / LLM applications** (text quality, relevance, safety)\n",
    "- **Classic ML models** (classification, regression)\n",
    "\n",
    "For agentic workflows, evaluations focus less on \"accuracy\" and more on **judgment-based metrics** that assess output quality.\n",
    "\n",
    "Key MLflow Agent Evaluation benefits:\n",
    "\n",
    "- Native support for **LLM-as-a-judge** evaluators\n",
    "- Row-level qualitative and quantitative results\n",
    "- Reproducible evaluation artifacts tied to prompts, models, and data\n",
    "\n",
    "In agent evaluations, the nature of evaluations shift from *‚ÄúIs this response good?‚Äù* rather than *‚ÄúIs this prediction correct?‚Äù*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e45dc4f1-1a0a-45c7-9379-ecb5c8114ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Where MLflow Evaluations Fit in a GenAI Workflow\n",
    "\n",
    "In GenAI applications, MLflow Evaluations typically happen:\n",
    "\n",
    "1. After prompt, chain, or agent changes\n",
    "2. Before deploying or updating a serving endpoint\n",
    "3. Continuously after deployment for quality regression detection\n",
    "\n",
    "They integrate with:\n",
    "\n",
    "- MLflow Experiments (prompt + model iteration)\n",
    "- Unity Catalog Model Registry (governed promotion)\n",
    "- Databricks Model Serving (endpoint-based evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32ddf6e4-c9a1-470e-bd2f-5b72734912f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](./_images/mlflow3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "816a98b6-ad3a-486b-94d7-8a49aa3c47ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Navigating through the user interface\n",
    "\n",
    "1. MLflow is available on Databricks out of the box\n",
    "2. Key Components of the MLflow interface consists of:\n",
    "-   Experiments\n",
    "-   Models\n",
    "-   Serving\n",
    "\n",
    "Let's first navigate to **Experiments** in the left-hand navigation\n",
    "\n",
    "![](./_images/experiments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d7c84e-61d6-4bb5-86d6-9672f68a2505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Running an Evaluation (High-Level Flow)\n",
    "\n",
    "At a high level, running an evaluation involves:\n",
    "\n",
    "- A trained model (or endpoint)\n",
    "- An evaluation dataset\n",
    "- An evaluation configuration (metrics, evaluators)\n",
    "\n",
    "MLflow handles metric computation and logging automatically. In Databricks, Tracing can be turned on with a single command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a65094f2-a02c-470b-bf75-6336d4d1207a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/julia.wu@databricks.com/ai_pioneer_2026/04_Agent_Evaluation/_images/tracing_log.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48080cf2-cd9b-4df3-82f7-b15de1b4b140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Evaluating a GenAI / LLM Application\n",
    "\n",
    "### What Is Being Evaluated?\n",
    "\n",
    "In GenAI, you are typically evaluating:\n",
    "\n",
    "- Prompt templates\n",
    "- Foundation models or fine-tuned LLMs\n",
    "- Chains, agents, or tools\n",
    "- End-to-end application outputs\n",
    "\n",
    "### Inputs to a GenAI Evaluation\n",
    "\n",
    "- Input prompts or questions\n",
    "- Optional reference answers\n",
    "- Model or endpoint outputs\n",
    "- One or more evaluators (LLM, rule-based, or human)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92523e70-8b3c-4250-bd45-a1fe3237c058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/julia.wu@databricks.com/ai_pioneer_2026/04_Agent_Evaluation/_images/tracing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1778683b-0f92-4902-9808-9cbe69d541b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Exploring Evaluation Results in the UI\n",
    "\n",
    "After the evaluation completes:\n",
    "\n",
    "1. Navigate to the **Experiment Runs** page\n",
    "2. Select the run associated with the evaluation\n",
    "3. Review:\n",
    "   - Metrics tab\n",
    "   - Artifacts tab\n",
    "\n",
    "üì∏ **Screenshot placeholder**: Run details page\n",
    "\n",
    "Key artifacts to highlight:\n",
    "\n",
    "- `evaluation_results.json`\n",
    "- Plots (confusion matrix, ROC, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e14e03d9-75b8-4084-b715-8c8ef77a1773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Model Comparison Using Evaluations\n",
    "\n",
    "Databricks allows you to compare evaluation results across runs.\n",
    "\n",
    "### Steps in the UI:\n",
    "\n",
    "1. Select multiple runs in the Experiment UI\n",
    "2. Click **Compare**\n",
    "3. Analyze metric differences side by side\n",
    "\n",
    "üìå *Talking point*: This is especially powerful when evaluations are standardized.\n",
    "\n",
    "üì∏ **Screenshot placeholder**: Run comparison view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ddffab-ec2b-4312-b2cd-4d9ae84963be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. GenAI Evaluators Explained\n",
    "\n",
    "MLflow supports multiple evaluator types for GenAI:\n",
    "\n",
    "### Automated LLM-as-a-Judge Evaluators\n",
    "\n",
    "- Answer relevance\n",
    "- Correctness (vs reference)\n",
    "- Faithfulness / groundedness\n",
    "\n",
    "### Safety & Policy Evaluators\n",
    "\n",
    "- Toxicity\n",
    "- Bias\n",
    "- Harmful content\n",
    "\n",
    "### Custom & Rule-Based Evaluators\n",
    "\n",
    "- Regex or keyword checks\n",
    "- Business logic validations\n",
    "\n",
    "üìå *Talking point*: Evaluators themselves can be models ‚Äî and should be versioned and tracked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b5fb65a-86c4-4310-9ba9-0ff618b5caf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. GenAI Evaluation Flow (UI-Oriented)\n",
    "\n",
    "Typical flow:\n",
    "\n",
    "1. Define prompts and expected outputs\n",
    "2. Run the model (endpoint or notebook)\n",
    "3. Trigger MLflow evaluation\n",
    "4. Review results in the experiment run\n",
    "\n",
    "üì∏ **Screenshot placeholder**: GenAI evaluation results table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6474dbd5-4f77-47e7-9fb0-55d8fc63c1f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Inspecting Per-Row Evaluation Results\n",
    "\n",
    "For GenAI evaluations, MLflow logs **row-level results**, including:\n",
    "\n",
    "- Model output\n",
    "- Reference output\n",
    "- Evaluation scores per row\n",
    "\n",
    "Where to find them:\n",
    "\n",
    "- Run Artifacts ‚Üí Evaluation tables\n",
    "- Downloadable JSON / Parquet artifacts\n",
    "\n",
    "üì∏ **Screenshot placeholder**: Per-row evaluation artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f29b9b3-46dd-48be-94dc-f527044d4984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 12. Using GenAI Evaluations for Governance & Promotion\n",
    "\n",
    "GenAI evaluations are critical for:\n",
    "\n",
    "- Justifying prompt or model changes\n",
    "- Preventing silent quality regressions\n",
    "- Enabling governed promotion to Production\n",
    "\n",
    "UI flow:\n",
    "\n",
    "1. Register the model or GenAI pipeline from the run\n",
    "2. View evaluation artifacts directly on the model version\n",
    "3. Use metrics and qualitative examples during stage reviews\n",
    "\n",
    "üìå *Talking point*: Evaluations provide **evidence**, not just scores.\n",
    "\n",
    "üì∏ **Screenshot placeholder**: Model Registry with evaluation artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f1af61-8d5e-4f34-bd7a-d521d1fa4ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 13. Best Practices\n",
    "\n",
    "**Standardize Evaluations**\n",
    "- Use consistent datasets and metrics\n",
    "\n",
    "**Log Early and Often**\n",
    "- Treat evaluations as first-class artifacts\n",
    "\n",
    "**Automate**\n",
    "- Include evaluations in CI/CD or training pipelines\n",
    "\n",
    "**Use Human Review Strategically**\n",
    "- Especially for GenAI edge cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f243417-023a-40ed-b7fb-f03df0bee0d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 14. Common GenAI Evaluation Pitfalls\n",
    "\n",
    "- Relying on a single LLM judge\n",
    "- Not versioning prompts or evaluation data\n",
    "- Evaluating only averages instead of per-row failures\n",
    "- Ignoring qualitative review for edge cases\n",
    "\n",
    "üìå *Talking point*: GenAI failures are often rare but severe ‚Äî averages can hide them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e264e3-5996-457c-a992-ad2725f001e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 15. GenAI vs Classic ML: Quick Comparison\n",
    "\n",
    "| Dimension | Classic ML | GenAI / LLMs |\n",
    "|--------|------------|--------------|\n",
    "| Primary signal | Numerical accuracy | Judgment-based quality |\n",
    "| Metrics | Accuracy, RMSE, AUC | Relevance, faithfulness, safety |\n",
    "| Ground truth | Required | Optional or partial |\n",
    "| Evaluation cost | Low | Higher (LLM calls) |\n",
    "| Failure modes | Gradual | Discrete / semantic |\n",
    "\n",
    "üìå *Talking point*: GenAI evaluation is closer to **code review** than test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e1eeb50-60e0-48dc-9719-5a108243134c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 16. Wrap-Up & Next Steps\n",
    "\n",
    "In this workshop, we covered:\n",
    "\n",
    "- How MLflow Evaluations support GenAI workflows\n",
    "- How GenAI evaluation differs from classic ML\n",
    "- Where to inspect qualitative and quantitative results in the UI\n",
    "- How to use evaluations for governance and deployment decisions\n",
    "\n",
    "**Next steps**:\n",
    "\n",
    "- Add GenAI evaluations to prompt iteration loops\n",
    "- Combine automated and human review\n",
    "- Use evaluation artifacts to gate production changes"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Agent_Eval",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
